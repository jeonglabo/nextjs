{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMARBE41b+2ZuBQ488uhtSh"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##**情報エントロピー (Information Entropy)**  \n","\n","### 情報とは何か？\n","\n","学習を進める中で、用語に引っかかることがよくあります。特に私の場合、統計学を学ぶ中で用語の壁を感じることが多かったように思います。例えば、「帰無仮説」といった用語はあまりにも聞き慣れないため、その意味を理解するのに苦労した良い例です。\n","\n","しかし、今回のポストで取り上げる「情報」という言葉は、日常的にあまりにも頻繁に使われているために、逆に学習の妨げとなる場合があると言えます。\n","\n","情報とは何でしょうか？「情報」はさまざまな視点から解釈できます。例えば、データベースの観点では、情報とはデータを利用しやすく加工したものを指します。この視点は主にコンピュータサイエンスの観点に基づいたもので、データを使用する側から見た情報の抽象的な概念と言えるでしょう。\n","\n","では、統計学では、数学的に情報とは何を意味するのでしょうか？統計学における情報量は「驚きの度合い」と言えるかもしれません。つまり、驚かされる内容ほど情報量が多いということです。この概念は確率の再解釈とも言え、確率が低い出来事ほど情報量が多いと解釈されます。なぜなら、ほとんど起こりえないことだからです。\n","\n","そこで、情報量を数式で定義するなら、おそらく確率値に反比例する値として定義するのが適切でしょう。\n","\n","つまり、ある出来事のランダム変数 $X$ に対して、情報量 (数式では Info と表現) の概念は以下のように考えられます。\n","\n","$$\n","\\text{Info} \\propto \\frac{1}{P(X)} \\quad \\tag{1}\n","$$\n","\n","### 情報量の定義\n","\n","より具体的に言うと、統計学での情報量は次のように定義されます。離散型ランダム変数 $X$ に対して、\n","\n","$$\n","I(x) = -\\log_b(P(X)) \\quad \\tag{2}\n","$$\n","\n","このとき、ログの底 $b$ は用途に応じて異なり、一般的には $b = 2, e, 10$ のいずれかが使われます (それぞれの単位は bit, nit, dit です)。\n","\n","情報量の定義にマイナスが付いているのは、次のようにログが情報量の定義に使用されているためです。\n","\n","$$\n","\\log_b\\left(\\frac{1}{P(X)}\\right) = -\\log_b(P(X)) \\quad \\tag{3}\n","$$\n","\n","情報量を定義する際にわざわざログを使っているのは、以下のような情報の概念を満たす数式である必要があるからです。\n","\n","1. 確率値 (または確率密度値) に反比例すること。  \n","2. 二つの出来事の情報量を合算すると、それぞれの情報量を足した値に等しくなること。\n","\n","### **エントロピー: 平均情報量**\n","\n","統計学では、エントロピーとは平均情報量を指します。冒頭で述べたように、エントロピーという用語を熱力学のエントロピーと結びつけようとはしないでください。統計学におけるエントロピーは、熱力学で使用される**ギブズ (Gibb's) エントロピー**の数式と似ているため、最終的には関連性を見つけられるかもしれませんが、ここでは新たな視点で情報エントロピーを考えましょう。\n","\n","情報エントロピー（またはシャノンエントロピー）は平均情報量です。離散型ランダム変数 $X$ のサンプル空間が $\\{x_1, x_2, \\ldots, x_n\\}$ であるとき、情報エントロピーは以下のように定義されます。\n","\n","$$\n","H(X) = E[I(X)] = -\\sum_{i=1}^{n} P(x_i) \\log_b(P(x_i)) \\quad \\tag{4}\n","$$\n","\n","ここで、$E[\\cdot]$ は期待値演算子を意味します。式 (4) がなぜ情報量の期待値 (つまり平均情報量) を意味するのか疑問に思う場合は、次の例を再考してみてください。\n","\n","サイコロ遊びをして、1から6までの目が出たとき、それぞれ 100円から 600円までを得ると考えましょう。このときの期待値は以下のようになります。\n","\n","$$\n","\\frac{1}{6} \\times 100 + \\frac{1}{6} \\times 200 + \\ldots + \\frac{1}{6} \\times 600 \\quad \\tag{5}\n","$$\n","\n","$$\n","= \\sum_{i=1}^{6} P(x_i) M(x_i) \\quad \\tag{6}\n","$$\n","\n","ここで、$M(x_i)$ は $x_i$ という出来事に対して得られる金額 (100円から600円) と考えます。つまり、期待値の定義は起こりうる出来事の確率 $\\times$ イベント値をすべて合計したものと考えられます。\n","\n","したがって、情報エントロピーは起こりうるすべての出来事の確率 $\\times$ 情報量の値を合計したものなので、\n","\n","$$\n","H(X) = \\sum_{i=1}^{n} P(x_i) (-\\log_b(P(x_i))) \\quad \\tag{6}\n","$$\n","\n","として計算して得られる値です。\n"],"metadata":{"id":"d4OxKTQ6pnw4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDs-tcO_pm5_"},"outputs":[],"source":[]}]}