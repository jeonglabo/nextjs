{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN6dx0/WqoZVStGabVa4EQ+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["このページは、主成分分析（PCA: Principal Component Analysis）についての説明です。PCAは、高次元データを低次元に変換することでデータの情報を効果的に保持しつつ、データの次元を削減する手法です。以下に内容をまとめます。\n","\n","### 主な内容このページでは、共分散行列（Covariance Matrix）の計算とその意味について説明しています。以下に、内容の要点をまとめます。\n","\n","### 主な内容\n","\n","1. **共分散行列の意義**:\n","   - 共分散行列は、データの特徴（feature）間の分散や相関関係を捉えます。データの分布を理解するために重要な役割を果たし、PCAなどの次元削減手法で利用されます。\n","   - $ n $ 個のデータと $ d $ 個の特徴がある場合、データ行列 $ X $ は $ n \\times d $ 次元になります。各特徴の平均値を 0 にする（標準化する）ために、データの平均を引きます。\n","\n","2. **データの中心化と標準化**:\n","   - 各特徴の平均値を 0 にするために、データ $ D $ からその平均 $ \\text{mean}(D) $ を引きます。この操作によって、中心化されたデータ行列 $ X $ が得られます。\n","\n","3. **共分散行列の計算方法**:\n","   - 共分散行列 $ X^T X $ を計算します。これは各特徴ベクトル同士の内積によって得られます。\n","   - 内積（dot product）は、各特徴間の類似度を示す尺度で、特徴同士がどれくらい関連しているかを示します。\n","\n","4. **共分散行列の具体例**:\n","   - 例として、体重と身長のデータが示され、それぞれの平均値（身長166、体重62）が計算されています。\n","   - 各データから平均を引き、中心化されたデータ行列 $ X $ を作成しています。\n","   - 最終的に $ X^T X $ によって共分散行列が求められます。この行列は、各特徴の分散とそれらの間の共分散を示します。\n","\n","5. **固有ベクトルと主軸の関係**:\n","   - 図で示されているように、データの広がりの方向（主軸）は固有ベクトルによって示されます。固有ベクトルはデータの分散が最大となる方向を指し、その方向への投影が主成分となります。\n","\n","### まとめ\n","共分散行列は、データの分布や相関を理解するための基盤となります。PCAのような次元削減手法では、この共分散行列を解析して、データの特徴間の最も重要な関係性を捉えます。\n","\n","1. **行列による変換の概要**:\n","   - 行列 $ A $ を用いて、ベクトル $ x $ を別のベクトル $ Ax $ に変換します。具体的な例として、語学と数学の点数データを行列を用いて変換する手法が示されています。\n","\n","2. **データの中心化と軸の最適化**:\n","   - データの平均を取ってデータを中心化します。\n","   - 変換されたデータの軸（主成分軸）を見つけるために、分散が最大となる方向を探します。これがPCAの中心的な考え方です。\n","\n","3. **共分散行列の役割**:\n","   - 共分散行列はデータの分散と特徴量間の相関を示します。これにより、データの「変動がどれくらい似ているか」を評価します。\n","   - 共分散行列を用いることで、最適な変換方向（主成分）が見つかります。\n","\n","4. **固有ベクトルと固有値**:\n","   - 共分散行列から固有ベクトルと固有値を計算し、データの最大の分散を持つ方向（主成分）を見つけます。\n","   - 固有ベクトルは、新しい基底を表し、その方向に沿ったデータの分布を示します。\n","\n","5. **データの可視化**:\n","   - 図示されているように、データは新しい基底（固有ベクトル）に沿って再配置され、次元削減が行われます。\n","   - 主成分軸に沿ってデータを射影することで、情報の損失を最小限に抑えつつ次元削減を実現します。\n","\n","### まとめ\n","PCAは、データの分散が最大となる方向を見つけ、データをその方向に射影することで、次元を削減します。この手法は、データの可視化、ノイズ除去、および機械学習モデルの性能向上に広く利用されています。\n"],"metadata":{"id":"7I8w7DAimrXQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lpRb4f9zmqcV"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["このページでは、共分散行列とその固有ベクトルの役割、分散（Variance）の最大化についての説明がされています。以下に内容を要約します。\n","\n","### 主な内容\n","\n","1. **共分散行列の構成**:\n","   - 共分散行列 $ X^T X $ はデータ行列 $ X $ の各ベクトル同士の内積から構成されます。行列の各要素は特徴間の共分散を示します。\n","   - データが多ければ多いほど、内積の値が大きくなり、より正確な共分散が得られます。\n","\n","2. **正規化された共分散行列**:\n","   - 共分散行列をデータの数 $ n $ で割ることで、標準化された共分散行列 $ \\Sigma = \\frac{1}{n} X^T X $ を得ます。これにより、各データ点の影響を均一化できます。\n","   - 例として、共分散行列の計算結果が $ \\Sigma = \\frac{1}{5} \\begin{bmatrix} 540 & 426 \\\\ 426 & 790 \\end{bmatrix} = \\begin{bmatrix} 108 & 85.2 \\\\ 85.2 & 158 \\end{bmatrix} $ となることが示されています。\n","\n","3. **分散の最大化と固有ベクトル**:\n","   - 共分散行列の固有ベクトルは、データの分散が最大となる方向（軸）を示します。データをこの方向に写像することで、分散を最大化することができます。\n","   - 分散の計算には、データベクトル $ X $ とその平均との偏差の二乗和が用いられます。\n","\n","4. **分散の数学的表現**:\n","   - 分散は $ \\text{Var}(X e) = \\frac{1}{n} (X e)^T (X e) $ と表され、これはさらに $ e^T \\Sigma e $ という形に展開できます。\n","   - 目的は、データを新しい軸（固有ベクトル）に写像したときの分散 $ e^T \\Sigma e $ を最大化することです。この最適化問題はラグランジュ乗数法を使って解かれます。\n","\n","5. **最適化問題の設定**:\n","   - 目的関数は分散を最大化すること、すなわち $ e^T \\Sigma e $ を最大にすることです。\n","   - 制約条件として $ |e| = 1 $（単位ベクトルの制約）を課しています。\n","   - ラグランジュ関数を設定し、微分を用いて分散の最大化の条件を求めています。\n","\n","### まとめ\n","このノートの内容は、PCA（主成分分析）の核となる部分であり、データの分散を最大化する方向を見つけるための数学的な手法を説明しています。これにより、次元削減やデータの特徴抽出において重要な軸を見つけることができます。\n"],"metadata":{"id":"94BAJ56zoWI-"}},{"cell_type":"markdown","source":["このページでは、主成分分析（PCA）における次元削減の考え方と、そのための固有値・固有ベクトルの役割について説明されています。\n","\n","### 主な内容\n","\n","1. **固有値と固有ベクトル**:\n","   - 共分散行列 $\\Sigma$ と固有ベクトル $\\vec{e}$ との関係は、$\\Sigma \\vec{e} = \\lambda \\vec{e}$ という固有値問題により示されます。ここで、$\\lambda$ は固有値、$\\vec{e}$ は対応する固有ベクトルです。\n","   - 分散 $ \\text{Var}(X \\vec{e}) $ は $\\lambda$ と等しくなり、固有値がデータの分散を最大化する方向を示すことが分かります。\n","\n","2. **次元削減の判断基準**:\n","   - 次元削減の目的は、データの情報を損なわずに必要な次元数を減らすことです。\n","   - データの共分散行列が full rank であると仮定し、固有値を降順に並べて、どこで次元を切り捨てるかを判断します。\n","   - 累積固有値の割合を用いて次元削減の基準を設定し、例えば、全体の固有値の合計に対して90%を説明できるようにすることが目標となります。\n","\n","3. **累積寄与率**:\n","   - 寄与率は各固有値 $\\lambda_i$ がデータ全体の分散にどれだけ寄与しているかを示す指標です。\n","   - 累積寄与率 $\\frac{\\sum_{i=1}^{k} \\lambda_i}{\\sum_{i=1}^{d} \\lambda_i}$ を求めることで、どの次元まで考慮すべきかを決めることができます。\n","\n","4. **スクリープロット（Scree Plot）**:\n","   - スクリープロットは、固有値を大きい順に並べ、どの次元まで情報を保持するかを視覚的に判断するためのツールです。\n","   - 図では、固有値が急激に減少する「ひざ（elbow）」のポイントで次元を切り捨てる判断が示されています。この例では、最初の3つの因子（次元）が重要であると判断されています。\n","\n","5. **次元削減の例**:\n","   - 累積寄与率が 90% を超えるポイントで次元を削減することで、3次元から2次元への次元削減が可能であると述べています。\n","   - これにより、全体のデータの情報を90%以上保持しつつ、計算量を減らすことができます。\n","\n","### まとめ\n","このページでは、固有値・固有ベクトルを用いた次元削減の基準と、その視覚的な判断方法としてのスクリープロットが説明されています。主成分分析（PCA）の実施において、どの次元まで考慮するかを合理的に決定するための方法論が示されています。\n"],"metadata":{"id":"r83ckBQFozPC"}}]}